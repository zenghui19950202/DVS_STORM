#!/usr/bin/env python
# -*- coding: UTF-8 -*-
# Author  ：zh , Date  ：2021/3/8 21:25
import numpy as np
import torch
import os
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from configparser import ConfigParser
from utils import load_configuration_parameters
import pandas as pd
from torchvision import transforms, utils
import cv2, glob
import matplotlib.pyplot as plt


class STORM_DVS(Dataset):
    """N-MNIST数据集初始化。如果没有预处理后的.npy文件，需要手动调用一次preprocessing函数进行预处理。
    Initial a N-MNIST dataset. Need to call the function preprocessing() first if there is no *.npy file generated by preprocessing().

    Args:
        train (bool): 是否为训练集。为False则为测试集。
            If "True", it means that's the traning set. Otherwise it's the testing set.
        step (int): 超参数step数。代表SNN模拟的时间步数。
            How many steps that SNNs will simulate.
        dt (int): 超参数dt，单位ms，代表一个时间步所代表的实际时间。N-MNIST数据集带有时间戳，所以需要此参数。
            How long does a step really required. It's needed because N-MNIST dataset includes time stamp.
        path (string, optional): 参数为预处理后的.npy数据文件路径时，可以预加载.npy文件。默认为None。
            Set this to the path of data file (with ".npy" suffix) to accelerate data loading. Default to None.
    """

    def __init__(self, train, win, path=None, net_model = 'SNN',Normalize = True):
        super(STORM_DVS, self).__init__()
        data_generation_parameters = load_configuration_parameters.load_data_generation_config_paras()
        self.image_size = data_generation_parameters['image_size']
        self.downsample_rate = data_generation_parameters['downsample_rate']
        self.net_model = net_model

        self.path = path
        self.train = train
        self.win = win
        self.DVS_image_size = int(self.image_size / self.downsample_rate)

        if self.train == True:
            self.data_path = self.path  + '/train'
        else:
            self.data_path = self.path  + '/valid'

        self.files = os.listdir(self.data_path)

        DVS_file_dir = os.path.join(self.data_path, '1.txt')
        f = open(DVS_file_dir, 'rb')
        raw_data = np.loadtxt(DVS_file_dir, dtype=np.float, delimiter=' ')
        f.close()
        raw_data = raw_data.astype(int)
        self.time_step = raw_data[:, 0].max() + 1
        self.dt = win / self.time_step


    def __len__(self):
        data_num = len(self.files) / 2
        return int(data_num)

    #  file number is data_num * 2(DVS_file and label file ) +1 (time window and step parameter file)
    def __getitem__(self, idx):
        """Dataset类的重载方法。返回指定idx位置的数据。可以在此处进行必要的预处理，但可能会拖慢性能。
        Override method. The data can be preprocessed here.

        Args:
            idx (int/Tensor/list): 数据index。
                Index of data.

        Returns:
            (x, y): 数据/标签组成的元组。
                A tuple with format: (data, label).
        """

        if torch.is_tensor(idx):
            idx = idx.tolist()

        DVS_file_dir = os.path.join(self.data_path, str(idx + 1) + '.txt')

        file_size = os.path.getsize(DVS_file_dir)

        if file_size == 0:
            pass
        else:
            f = open(DVS_file_dir, 'rb')
            raw_data = np.loadtxt(DVS_file_dir, dtype=np.float, delimiter=' ')
            f.close()
            raw_data = raw_data.astype(int)
            data = np.zeros(shape=(1, self.DVS_image_size, self.DVS_image_size, self.time_step))
            # TODO the image_size should be calculated by downsample rate rather than loading by configuration file
            for i in range(raw_data.shape[0]):
                # index = int(win_indices[i])
                index = i
                polar = 0  # Drop the polar
                data[0, raw_data[index, 1], raw_data[index, 2], raw_data[index, 0]] = 1
                # 1 for an event, 0 for nothing

        label_file_dir = os.path.join(self.data_path, str(idx + 1) + '_label.txt')
        label_file = open(label_file_dir, 'r')
        label_loc = np.loadtxt(label_file, dtype=np.int, delimiter=' ')
        label_file.close()
        label = np.zeros(shape=(self.image_size, self.image_size))
        for i in range(label_loc.shape[0]):
            label[label_loc[i, 0], label_loc[i, 1]] = 1

        if self.net_model != 'SNN':
            data = data.mean(axis = 3)

        # plot to help debug
        eventflow_sum = data.sum(axis = 3).squeeze()
        plt.subplot(121), plt.imshow(eventflow_sum)
        plt.subplot(122), plt.imshow(label*255)
        plt.show()

        return torch.from_numpy(data), torch.from_numpy(label)


if __name__ == '__main__':
    data_generation_parameters = load_configuration_parameters.load_data_generation_config_paras()
    # config = ConfigParser()
    # config.read('../configuration.ini')
    output_directory = data_generation_parameters['output_directory']
    output_directory = '/data/zh/DVS_STORM_sample_data/'

    STORM_DVS_dataset = STORM_DVS(train=True, win=100, path=output_directory)
    SIM_train_dataloader = DataLoader(STORM_DVS_dataset, batch_size=1, shuffle=True)
    # STORM_DVS_dataset[0]
    for i, (data, label) in enumerate(SIM_train_dataloader):
         print(i)

         if i>0:
             break
